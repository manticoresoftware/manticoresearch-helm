# Default values for manticoresearch.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName
#   storageClass: myStorageClass
  manticoresearch: {}

balancer:
  replicaCount: 1
  # priorityClassName: ""
  runInterval: 5
  extraPackages: true
  image:
    repository: manticoresearch/helm-balancer
#    tag: 3.6.0.0
    pullPolicy: IfNotPresent
  service:
    ql:
      port: 9306
      targetPort: 9306
    observer:
      port: 8080
      targetPort: 8080
    http:
      port: 9308
      targetPort: 9308
    binary:
      port: 9312
      targetPort: 9312
  config:
    path: /mnt/configmap.conf
    index_ha_strategy: nodeads
    content: |
      searchd
      {
        listen = /var/run/mysqld/mysqld.sock:mysql
        listen = 9306:mysql
        listen = 9308:http
        log = /dev/stdout
        query_log = /dev/stdout
        query_log_format = sphinxql
        pid_file = /var/run/manticore/searchd.pid
        binlog_path = /var/lib/manticore
        shutdown_timeout = 25s
        auto_optimize = 0
      }
  volumeMounts: []
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
  #   cpu: 100m
  #   memory: 128Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}

worker:
  replicaCount: 3
  # priorityClassName: ""
  clusterName: manticore
  ## In the `multi-master` mode it doesn't matter to which node you write and from what node you read. This is a simpler and more efficient approach, but in case of an emergency cluster shutdown when all the nodes are down at the same time you have to recover the cluster  manually.

  ## The `master-slave` mode expects that you write only to node 0, so it's guaranteed to always have the most actual data. Then in case the cluster is fully shut down and then back up all other nodes will connect to the node 0.
  replicationMode: multi-master
  ## Allows automatic restore of a replication cluster if it is broken due to no quorum. Recommended with the master-slave `replicationMode`. May be dangerous in terms of data loss in the master-master `replicationMode`.
  quorumRecovery: false
  ## interval in seconds to run cluster status check
  extraPackages: true
  quorumCheckInterval: 15
  autoAddTablesInCluster: true
  logLevel: INFO
  image:
    repository: manticoresearch/helm-worker
#    tag: 3.6.0.0
    pullPolicy: IfNotPresent
  service:
    ql:
      port: 9306
      targetPort: 9306
    http:
      port: 9308
      targetPort: 9308
    binary:
      port: 9312
      targetPort: 9312
  persistence:
    enabled: true
    ## redis data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessModes:
      - ReadWriteOnce
    size: 1Gi
    ## Persistent Volume selectors
    ## https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector
    matchLabels: {}
    matchExpressions: {}
#  volume:
#    size: 1Gi
#    storageClassName: false
  config:
    path: /mnt/manticore.conf
    content: |
      searchd
      {
        listen = /var/run/mysqld/mysqld.sock:mysql41
        listen = 9306:mysql41
        listen = 9308:http
        listen = 9301:mysql_vip
        listen = $hostname:9312
        listen = $hostname:9315-9415:replication
        node_address = $hostname
        binlog_path = /var/lib/manticore
        log = /dev/stdout
        query_log = /dev/stdout
        query_log_format = sphinxql
        pid_file = /var/run/manticore/searchd.pid
        data_dir = /var/lib/manticore
        shutdown_timeout = 25s
        auto_optimize = 0
      }
  volumeMounts: []
  resources: {}
  # See comment on balancer.resources
  nodeSelector: {}
  tolerations: []
  affinity: {}

exporter:
  enabled: false
  image:
    repository: manticoresearch/prometheus-exporter
    pullPolicy: IfNotPresent
    tag: 5.0.2.5
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8081"
    prometheus.io/scrape: "true"
  resources: {}
  # See comment on balancer.resources

serviceMonitor:
  # When set to true then use a ServiceMonitor to collect metrics
  enabled: false
  # Alternative namespace for ServiceMonitor resources
  #namespace:
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set timeout for scrape
  scrapeTimeout: 10s

optimize:
  enabled: true
  interval: "30"
  coefficient: "2"

imagePullSecrets: []
nameOverride: ""
fullNameOverride: ""

serviceAccount:
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "manticore-sa"

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

nodeSelector: {}

tolerations: []

affinity: {}

persistence: {}
  ## A manually managed Persistent Volume and Claim
  ## Requires persistence.enabled: true
  ## If defined, PVC must be created manually before volume will be bound
# existingClaim:
